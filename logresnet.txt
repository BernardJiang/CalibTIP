step 1: 
step 2: 
Layer conv1, precision switch from w8a8 to w4a8.
Layer layer1.0.conv1, precision switch from w8a8 to w4a8.
Layer layer1.0.conv2, precision switch from w8a8 to w4a8.
Layer layer1.0.conv3, precision switch from w8a8 to w4a8.
Layer layer1.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer1.1.conv1, precision switch from w8a8 to w4a8.
Layer layer1.1.conv2, precision switch from w8a8 to w4a8.
Layer layer1.1.conv3, precision switch from w8a8 to w4a8.
Layer layer1.2.conv1, precision switch from w8a8 to w4a8.
Layer layer1.2.conv2, precision switch from w8a8 to w4a8.
Layer layer1.2.conv3, precision switch from w8a8 to w4a8.
Layer layer2.0.conv1, precision switch from w8a8 to w4a8.
Layer layer2.0.conv2, precision switch from w8a8 to w4a8.
Layer layer2.0.conv3, precision switch from w8a8 to w4a8.
Layer layer2.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer2.1.conv1, precision switch from w8a8 to w4a8.
Layer layer2.1.conv2, precision switch from w8a8 to w4a8.
Layer layer2.1.conv3, precision switch from w8a8 to w4a8.
Layer layer2.2.conv1, precision switch from w8a8 to w4a8.
Layer layer2.2.conv2, precision switch from w8a8 to w4a8.
Layer layer2.2.conv3, precision switch from w8a8 to w4a8.
Layer layer2.3.conv1, precision switch from w8a8 to w4a8.
Layer layer2.3.conv2, precision switch from w8a8 to w4a8.
Layer layer2.3.conv3, precision switch from w8a8 to w4a8.
Layer layer3.0.conv1, precision switch from w8a8 to w4a8.
Layer layer3.0.conv2, precision switch from w8a8 to w4a8.
Layer layer3.0.conv3, precision switch from w8a8 to w4a8.
Layer layer3.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer3.1.conv1, precision switch from w8a8 to w4a8.
Layer layer3.1.conv2, precision switch from w8a8 to w4a8.
Layer layer3.1.conv3, precision switch from w8a8 to w4a8.
Layer layer3.2.conv1, precision switch from w8a8 to w4a8.
Layer layer3.2.conv2, precision switch from w8a8 to w4a8.
Layer layer3.2.conv3, precision switch from w8a8 to w4a8.
Layer layer3.3.conv1, precision switch from w8a8 to w4a8.
Layer layer3.3.conv2, precision switch from w8a8 to w4a8.
Layer layer3.3.conv3, precision switch from w8a8 to w4a8.
Layer layer3.4.conv1, precision switch from w8a8 to w4a8.
Layer layer3.4.conv2, precision switch from w8a8 to w4a8.
Layer layer3.4.conv3, precision switch from w8a8 to w4a8.
Layer layer3.5.conv1, precision switch from w8a8 to w4a8.
Layer layer3.5.conv2, precision switch from w8a8 to w4a8.
Layer layer3.5.conv3, precision switch from w8a8 to w4a8.
Layer layer4.0.conv1, precision switch from w8a8 to w4a8.
Layer layer4.0.conv2, precision switch from w8a8 to w4a8.
Layer layer4.0.conv3, precision switch from w8a8 to w4a8.
Layer layer4.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer4.1.conv1, precision switch from w8a8 to w4a8.
Layer layer4.1.conv2, precision switch from w8a8 to w4a8.
Layer layer4.1.conv3, precision switch from w8a8 to w4a8.
Layer layer4.2.conv1, precision switch from w8a8 to w4a8.
Layer layer4.2.conv2, precision switch from w8a8 to w4a8.
Layer layer4.2.conv3, precision switch from w8a8 to w4a8.
Layer fc, precision switch from w8a8 to w4a8.
read json file /workspace/develop/CalibTIP/results/resnet50_w4a8.adaquant/resnet_adaquant.piano.kdp530.scaled.onnx.json
Json : Layer conv1, precision switch from w8a8 to w8a8.
Json : Layer layer1.0.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer1.0.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer1.0.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer1.0.downsample.0, precision switch from w4a8 to w4a8.
Json : Layer layer1.1.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer1.1.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer1.1.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer1.2.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer1.2.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer1.2.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer2.0.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer2.0.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer2.0.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer2.0.downsample.0, precision switch from w4a8 to w4a8.
Json : Layer layer2.1.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer2.1.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer2.1.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer2.2.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer2.2.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer2.2.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer2.3.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer2.3.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer2.3.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer3.0.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer3.0.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer3.0.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer3.0.downsample.0, precision switch from w4a8 to w4a8.
Json : Layer layer3.1.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer3.1.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer3.1.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer3.2.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer3.2.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer3.2.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer3.3.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer3.3.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer3.3.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer3.4.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer3.4.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer3.4.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer3.5.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer3.5.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer3.5.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer4.0.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer4.0.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer4.0.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer4.0.downsample.0, precision switch from w4a8 to w4a8.
Json : Layer layer4.1.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer4.1.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer4.1.conv3, precision switch from w4a8 to w4a8.
Json : Layer layer4.2.conv1, precision switch from w4a8 to w4a8.
Json : Layer layer4.2.conv2, precision switch from w4a8 to w4a8.
Json : Layer layer4.2.conv3, precision switch from w4a8 to w4a8.
Json : Layer fc, precision switch from w4a8 to w4a8.
step 3: 
Layer conv1, precision switch from w8a8 to w4a8.
Layer layer1.0.conv1, precision switch from w8a8 to w4a8.
Layer layer1.0.conv2, precision switch from w8a8 to w4a8.
Layer layer1.0.conv3, precision switch from w8a8 to w4a8.
Layer layer1.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer1.1.conv1, precision switch from w8a8 to w4a8.
Layer layer1.1.conv2, precision switch from w8a8 to w4a8.
Layer layer1.1.conv3, precision switch from w8a8 to w4a8.
Layer layer1.2.conv1, precision switch from w8a8 to w4a8.
Layer layer1.2.conv2, precision switch from w8a8 to w4a8.
Layer layer1.2.conv3, precision switch from w8a8 to w4a8.
Layer layer2.0.conv1, precision switch from w8a8 to w4a8.
Layer layer2.0.conv2, precision switch from w8a8 to w4a8.
Layer layer2.0.conv3, precision switch from w8a8 to w4a8.
Layer layer2.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer2.1.conv1, precision switch from w8a8 to w4a8.
Layer layer2.1.conv2, precision switch from w8a8 to w4a8.
Layer layer2.1.conv3, precision switch from w8a8 to w4a8.
Layer layer2.2.conv1, precision switch from w8a8 to w4a8.
Layer layer2.2.conv2, precision switch from w8a8 to w4a8.
Layer layer2.2.conv3, precision switch from w8a8 to w4a8.
Layer layer2.3.conv1, precision switch from w8a8 to w4a8.
Layer layer2.3.conv2, precision switch from w8a8 to w4a8.
Layer layer2.3.conv3, precision switch from w8a8 to w4a8.
Layer layer3.0.conv1, precision switch from w8a8 to w4a8.
Layer layer3.0.conv2, precision switch from w8a8 to w4a8.
Layer layer3.0.conv3, precision switch from w8a8 to w4a8.
Layer layer3.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer3.1.conv1, precision switch from w8a8 to w4a8.
Layer layer3.1.conv2, precision switch from w8a8 to w4a8.
Layer layer3.1.conv3, precision switch from w8a8 to w4a8.
Layer layer3.2.conv1, precision switch from w8a8 to w4a8.
Layer layer3.2.conv2, precision switch from w8a8 to w4a8.
Layer layer3.2.conv3, precision switch from w8a8 to w4a8.
Layer layer3.3.conv1, precision switch from w8a8 to w4a8.
Layer layer3.3.conv2, precision switch from w8a8 to w4a8.
Layer layer3.3.conv3, precision switch from w8a8 to w4a8.
Layer layer3.4.conv1, precision switch from w8a8 to w4a8.
Layer layer3.4.conv2, precision switch from w8a8 to w4a8.
Layer layer3.4.conv3, precision switch from w8a8 to w4a8.
Layer layer3.5.conv1, precision switch from w8a8 to w4a8.
Layer layer3.5.conv2, precision switch from w8a8 to w4a8.
Layer layer3.5.conv3, precision switch from w8a8 to w4a8.
Layer layer4.0.conv1, precision switch from w8a8 to w4a8.
Layer layer4.0.conv2, precision switch from w8a8 to w4a8.
Layer layer4.0.conv3, precision switch from w8a8 to w4a8.
Layer layer4.0.downsample.0, precision switch from w8a8 to w4a8.
Layer layer4.1.conv1, precision switch from w8a8 to w4a8.
Layer layer4.1.conv2, precision switch from w8a8 to w4a8.
Layer layer4.1.conv3, precision switch from w8a8 to w4a8.
Layer layer4.2.conv1, precision switch from w8a8 to w4a8.
Layer layer4.2.conv2, precision switch from w8a8 to w4a8.
Layer layer4.2.conv3, precision switch from w8a8 to w4a8.
Layer fc, precision switch from w8a8 to w4a8.
Input/outputs cached

Starting ONNX export with onnx 1.11.0...
****onnx file**** /workspace/develop/CalibTIP/results/resnet50_w4a8.adaquant/resnet.absorb_bn.measure_perC.before_adaquant.onnx
ONNX export success, saved as /workspace/develop/CalibTIP/results/resnet50_w4a8.adaquant/resnet.absorb_bn.measure_perC.before_adaquant.onnx
Bernard calculate float point model accuracy before enabling quantization!
Bernard calculate fixed point model accuracy before training!

Optimize 0:conv1 for w4a8 bit of shape torch.Size([64, 3, 7, 7])

MSE before adaquant: 7.240036e-05  RELU True
MSE after  adaquant: 2.815377e-05
 End of training layer  conv1 better/total layers= 1 / 1 


Optimize 1:layer1.0.conv1 for w4a8 bit of shape torch.Size([64, 64, 1, 1])

MSE before adaquant: 3.527072e-03  RELU True
MSE after  adaquant: 3.219396e-03
 End of training layer  layer1.0.conv1 better/total layers= 2 / 2 


Optimize 2:layer1.0.conv2 for w4a8 bit of shape torch.Size([64, 64, 3, 3])

MSE before adaquant: 2.955772e-03  RELU True
MSE after  adaquant: 2.262008e-03
 End of training layer  layer1.0.conv2 better/total layers= 3 / 3 


Optimize 3:layer1.0.conv3 for w4a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 3.211243e-03  RELU False
MSE after  adaquant: 3.077920e-03
 End of training layer  layer1.0.conv3 better/total layers= 4 / 4 


Optimize 4:layer1.0.downsample.0 for w4a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 1.468883e-02  RELU False
MSE after  adaquant: 1.312661e-02
 End of training layer  layer1.0.downsample.0 better/total layers= 5 / 5 


Optimize 5:layer1.1.conv1 for w4a8 bit of shape torch.Size([64, 256, 1, 1])

MSE before adaquant: 1.619302e-03  RELU True
MSE after  adaquant: 1.313771e-03
 End of training layer  layer1.1.conv1 better/total layers= 6 / 6 


Optimize 6:layer1.1.conv2 for w4a8 bit of shape torch.Size([64, 64, 3, 3])

MSE before adaquant: 2.870011e-03  RELU True
MSE after  adaquant: 2.368678e-03
 End of training layer  layer1.1.conv2 better/total layers= 7 / 7 


Optimize 7:layer1.1.conv3 for w4a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 8.401116e-04  RELU False
MSE after  adaquant: 7.807052e-04
 End of training layer  layer1.1.conv3 better/total layers= 8 / 8 


Optimize 8:layer1.2.conv1 for w4a8 bit of shape torch.Size([64, 256, 1, 1])

MSE before adaquant: 1.706956e-03  RELU True
MSE after  adaquant: 1.430598e-03
 End of training layer  layer1.2.conv1 better/total layers= 9 / 9 


Optimize 9:layer1.2.conv2 for w4a8 bit of shape torch.Size([64, 64, 3, 3])

MSE before adaquant: 1.089192e-03  RELU True
MSE after  adaquant: 8.301935e-04
 End of training layer  layer1.2.conv2 better/total layers= 10 / 10 


Optimize 10:layer1.2.conv3 for w4a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 6.205440e-04  RELU False
MSE after  adaquant: 5.868069e-04
 End of training layer  layer1.2.conv3 better/total layers= 11 / 11 


Optimize 11:layer2.0.conv1 for w4a8 bit of shape torch.Size([128, 256, 1, 1])

MSE before adaquant: 1.780609e-03  RELU True
MSE after  adaquant: 1.373156e-03
 End of training layer  layer2.0.conv1 better/total layers= 12 / 12 


Optimize 12:layer2.0.conv2 for w4a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 7.591352e-04  RELU True
MSE after  adaquant: 5.428113e-04
 End of training layer  layer2.0.conv2 better/total layers= 13 / 13 


Optimize 13:layer2.0.conv3 for w4a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 1.789926e-03  RELU False
MSE after  adaquant: 1.644108e-03
 End of training layer  layer2.0.conv3 better/total layers= 14 / 14 


Optimize 14:layer2.0.downsample.0 for w4a8 bit of shape torch.Size([512, 256, 1, 1])

MSE before adaquant: 3.148779e-03  RELU False
MSE after  adaquant: 2.599498e-03
 End of training layer  layer2.0.downsample.0 better/total layers= 15 / 15 


Optimize 15:layer2.1.conv1 for w4a8 bit of shape torch.Size([128, 512, 1, 1])

MSE before adaquant: 5.868605e-04  RELU True
MSE after  adaquant: 3.916807e-04
 End of training layer  layer2.1.conv1 better/total layers= 16 / 16 


Optimize 16:layer2.1.conv2 for w4a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 5.191118e-03  RELU True
MSE after  adaquant: 3.560126e-03
 End of training layer  layer2.1.conv2 better/total layers= 17 / 17 


Optimize 17:layer2.1.conv3 for w4a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 1.177785e-03  RELU False
MSE after  adaquant: 1.126360e-03
 End of training layer  layer2.1.conv3 better/total layers= 18 / 18 


Optimize 18:layer2.2.conv1 for w4a8 bit of shape torch.Size([128, 512, 1, 1])

MSE before adaquant: 1.141840e-03  RELU True
MSE after  adaquant: 7.528387e-04
 End of training layer  layer2.2.conv1 better/total layers= 19 / 19 


Optimize 19:layer2.2.conv2 for w4a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 1.390115e-03  RELU True
MSE after  adaquant: 9.237113e-04
 End of training layer  layer2.2.conv2 better/total layers= 20 / 20 


Optimize 20:layer2.2.conv3 for w4a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 9.588349e-04  RELU False
MSE after  adaquant: 8.917745e-04
 End of training layer  layer2.2.conv3 better/total layers= 21 / 21 


Optimize 21:layer2.3.conv1 for w4a8 bit of shape torch.Size([128, 512, 1, 1])

MSE before adaquant: 8.605977e-04  RELU True
MSE after  adaquant: 6.188766e-04
 End of training layer  layer2.3.conv1 better/total layers= 22 / 22 


Optimize 22:layer2.3.conv2 for w4a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 7.583139e-04  RELU True
MSE after  adaquant: 5.818821e-04
 End of training layer  layer2.3.conv2 better/total layers= 23 / 23 


Optimize 23:layer2.3.conv3 for w4a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 8.311208e-04  RELU False
MSE after  adaquant: 7.816387e-04
 End of training layer  layer2.3.conv3 better/total layers= 24 / 24 


Optimize 24:layer3.0.conv1 for w4a8 bit of shape torch.Size([256, 512, 1, 1])

MSE before adaquant: 1.458192e-03  RELU True
MSE after  adaquant: 1.234261e-03
 End of training layer  layer3.0.conv1 better/total layers= 25 / 25 


Optimize 25:layer3.0.conv2 for w4a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 1.117487e-03  RELU True
MSE after  adaquant: 8.713147e-04
 End of training layer  layer3.0.conv2 better/total layers= 26 / 26 


Optimize 26:layer3.0.conv3 for w4a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 2.044435e-03  RELU False
MSE after  adaquant: 1.745890e-03
 End of training layer  layer3.0.conv3 better/total layers= 27 / 27 


Optimize 27:layer3.0.downsample.0 for w4a8 bit of shape torch.Size([1024, 512, 1, 1])

MSE before adaquant: 1.143051e-03  RELU False
MSE after  adaquant: 9.091328e-04
 End of training layer  layer3.0.downsample.0 better/total layers= 28 / 28 


Optimize 28:layer3.1.conv1 for w4a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 8.327510e-04  RELU True
MSE after  adaquant: 6.003495e-04
 End of training layer  layer3.1.conv1 better/total layers= 29 / 29 


Optimize 29:layer3.1.conv2 for w4a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 1.171453e-03  RELU True
MSE after  adaquant: 8.283372e-04
 End of training layer  layer3.1.conv2 better/total layers= 30 / 30 


Optimize 30:layer3.1.conv3 for w4a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 1.352714e-03  RELU False
MSE after  adaquant: 1.279686e-03
 End of training layer  layer3.1.conv3 better/total layers= 31 / 31 


Optimize 31:layer3.2.conv1 for w4a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 8.972663e-04  RELU True
MSE after  adaquant: 6.417923e-04
 End of training layer  layer3.2.conv1 better/total layers= 32 / 32 


Optimize 32:layer3.2.conv2 for w4a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 5.016664e-04  RELU True
MSE after  adaquant: 3.938860e-04
 End of training layer  layer3.2.conv2 better/total layers= 33 / 33 


Optimize 33:layer3.2.conv3 for w4a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 7.532880e-04  RELU False
MSE after  adaquant: 7.160587e-04
 End of training layer  layer3.2.conv3 better/total layers= 34 / 34 


Optimize 34:layer3.3.conv1 for w4a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 8.182314e-04  RELU True
MSE after  adaquant: 5.979362e-04
 End of training layer  layer3.3.conv1 better/total layers= 35 / 35 


Optimize 35:layer3.3.conv2 for w4a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 7.675213e-04  RELU True
MSE after  adaquant: 6.384437e-04
 End of training layer  layer3.3.conv2 better/total layers= 36 / 36 


Optimize 36:layer3.3.conv3 for w4a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 9.359827e-04  RELU False
MSE after  adaquant: 9.008749e-04
 End of training layer  layer3.3.conv3 better/total layers= 37 / 37 


Optimize 37:layer3.4.conv1 for w4a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 8.201646e-04  RELU True
MSE after  adaquant: 6.529010e-04
 End of training layer  layer3.4.conv1 better/total layers= 38 / 38 


Optimize 38:layer3.4.conv2 for w4a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 9.766485e-04  RELU True
MSE after  adaquant: 8.029693e-04
 End of training layer  layer3.4.conv2 better/total layers= 39 / 39 


Optimize 39:layer3.4.conv3 for w4a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 1.191495e-03  RELU False
MSE after  adaquant: 1.140835e-03
 End of training layer  layer3.4.conv3 better/total layers= 40 / 40 


Optimize 40:layer3.5.conv1 for w4a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 9.850722e-04  RELU True
MSE after  adaquant: 8.450070e-04
 End of training layer  layer3.5.conv1 better/total layers= 41 / 41 


Optimize 41:layer3.5.conv2 for w4a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 1.390589e-03  RELU True
MSE after  adaquant: 1.132171e-03
 End of training layer  layer3.5.conv2 better/total layers= 42 / 42 


Optimize 42:layer3.5.conv3 for w4a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 1.270643e-03  RELU False
MSE after  adaquant: 1.220009e-03
 End of training layer  layer3.5.conv3 better/total layers= 43 / 43 


Optimize 43:layer4.0.conv1 for w4a8 bit of shape torch.Size([512, 1024, 1, 1])

MSE before adaquant: 8.541473e-04  RELU True
MSE after  adaquant: 7.844506e-04
 End of training layer  layer4.0.conv1 better/total layers= 44 / 44 


Optimize 44:layer4.0.conv2 for w4a8 bit of shape torch.Size([512, 512, 3, 3])

MSE before adaquant: 7.096843e-04  RELU True
MSE after  adaquant: 6.340930e-04
 End of training layer  layer4.0.conv2 better/total layers= 45 / 45 


Optimize 45:layer4.0.conv3 for w4a8 bit of shape torch.Size([2048, 512, 1, 1])

MSE before adaquant: 1.697165e-02  RELU False
MSE after  adaquant: 1.647429e-02
 End of training layer  layer4.0.conv3 better/total layers= 46 / 46 


Optimize 46:layer4.0.downsample.0 for w4a8 bit of shape torch.Size([2048, 1024, 1, 1])

MSE before adaquant: 9.929293e-03  RELU False
MSE after  adaquant: 9.466494e-03
 End of training layer  layer4.0.downsample.0 better/total layers= 47 / 47 


Optimize 47:layer4.1.conv1 for w4a8 bit of shape torch.Size([512, 2048, 1, 1])

MSE before adaquant: 1.064168e-03  RELU True
MSE after  adaquant: 5.004721e-04
 End of training layer  layer4.1.conv1 better/total layers= 48 / 48 


Optimize 48:layer4.1.conv2 for w4a8 bit of shape torch.Size([512, 512, 3, 3])

MSE before adaquant: 5.348509e-04  RELU True
MSE after  adaquant: 4.821656e-04
 End of training layer  layer4.1.conv2 better/total layers= 49 / 49 


Optimize 49:layer4.1.conv3 for w4a8 bit of shape torch.Size([2048, 512, 1, 1])

MSE before adaquant: 1.471153e-02  RELU False
MSE after  adaquant: 1.447921e-02
 End of training layer  layer4.1.conv3 better/total layers= 50 / 50 


Optimize 50:layer4.2.conv1 for w4a8 bit of shape torch.Size([512, 2048, 1, 1])

MSE before adaquant: 1.191993e-03  RELU True
MSE after  adaquant: 6.759643e-04
 End of training layer  layer4.2.conv1 better/total layers= 51 / 51 


Optimize 51:layer4.2.conv2 for w4a8 bit of shape torch.Size([512, 512, 3, 3])

MSE before adaquant: 7.457865e-04  RELU True
MSE after  adaquant: 6.644619e-04
 End of training layer  layer4.2.conv2 better/total layers= 52 / 52 


Optimize 52:layer4.2.conv3 for w4a8 bit of shape torch.Size([2048, 512, 1, 1])

MSE before adaquant: 7.846126e-02  RELU False
MSE after  adaquant: 7.706340e-02
 End of training layer  layer4.2.conv3 better/total layers= 53 / 53 


Optimize 53:fc for w4a8 bit of shape torch.Size([1000, 2048])

MSE before adaquant: 9.512427e+00  RELU False
MSE after  adaquant: 8.111405e+00
 End of training layer  fc better/total layers= 54 / 54 

step 1: 
step 2: 
Layer conv1, precision switch from w8a8 to w8a8.
Layer layer1.0.conv1, precision switch from w8a8 to w8a8.
Layer layer1.0.conv2, precision switch from w8a8 to w8a8.
Layer layer1.0.conv3, precision switch from w8a8 to w8a8.
Layer layer1.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer1.1.conv1, precision switch from w8a8 to w8a8.
Layer layer1.1.conv2, precision switch from w8a8 to w8a8.
Layer layer1.1.conv3, precision switch from w8a8 to w8a8.
Layer layer1.2.conv1, precision switch from w8a8 to w8a8.
Layer layer1.2.conv2, precision switch from w8a8 to w8a8.
Layer layer1.2.conv3, precision switch from w8a8 to w8a8.
Layer layer2.0.conv1, precision switch from w8a8 to w8a8.
Layer layer2.0.conv2, precision switch from w8a8 to w8a8.
Layer layer2.0.conv3, precision switch from w8a8 to w8a8.
Layer layer2.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer2.1.conv1, precision switch from w8a8 to w8a8.
Layer layer2.1.conv2, precision switch from w8a8 to w8a8.
Layer layer2.1.conv3, precision switch from w8a8 to w8a8.
Layer layer2.2.conv1, precision switch from w8a8 to w8a8.
Layer layer2.2.conv2, precision switch from w8a8 to w8a8.
Layer layer2.2.conv3, precision switch from w8a8 to w8a8.
Layer layer2.3.conv1, precision switch from w8a8 to w8a8.
Layer layer2.3.conv2, precision switch from w8a8 to w8a8.
Layer layer2.3.conv3, precision switch from w8a8 to w8a8.
Layer layer3.0.conv1, precision switch from w8a8 to w8a8.
Layer layer3.0.conv2, precision switch from w8a8 to w8a8.
Layer layer3.0.conv3, precision switch from w8a8 to w8a8.
Layer layer3.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer3.1.conv1, precision switch from w8a8 to w8a8.
Layer layer3.1.conv2, precision switch from w8a8 to w8a8.
Layer layer3.1.conv3, precision switch from w8a8 to w8a8.
Layer layer3.2.conv1, precision switch from w8a8 to w8a8.
Layer layer3.2.conv2, precision switch from w8a8 to w8a8.
Layer layer3.2.conv3, precision switch from w8a8 to w8a8.
Layer layer3.3.conv1, precision switch from w8a8 to w8a8.
Layer layer3.3.conv2, precision switch from w8a8 to w8a8.
Layer layer3.3.conv3, precision switch from w8a8 to w8a8.
Layer layer3.4.conv1, precision switch from w8a8 to w8a8.
Layer layer3.4.conv2, precision switch from w8a8 to w8a8.
Layer layer3.4.conv3, precision switch from w8a8 to w8a8.
Layer layer3.5.conv1, precision switch from w8a8 to w8a8.
Layer layer3.5.conv2, precision switch from w8a8 to w8a8.
Layer layer3.5.conv3, precision switch from w8a8 to w8a8.
Layer layer4.0.conv1, precision switch from w8a8 to w8a8.
Layer layer4.0.conv2, precision switch from w8a8 to w8a8.
Layer layer4.0.conv3, precision switch from w8a8 to w8a8.
Layer layer4.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer4.1.conv1, precision switch from w8a8 to w8a8.
Layer layer4.1.conv2, precision switch from w8a8 to w8a8.
Layer layer4.1.conv3, precision switch from w8a8 to w8a8.
Layer layer4.2.conv1, precision switch from w8a8 to w8a8.
Layer layer4.2.conv2, precision switch from w8a8 to w8a8.
Layer layer4.2.conv3, precision switch from w8a8 to w8a8.
Layer fc, precision switch from w8a8 to w8a8.
read json file /workspace/develop/CalibTIP/results/resnet50_w8a8.adaquant/resnet_adaquant.piano.kdp530.scaled.onnx.json
Json : Layer conv1, precision switch from w8a8 to w8a8.
Json : Layer layer1.0.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer1.0.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer1.0.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer1.0.downsample.0, precision switch from w8a8 to w8a8.
Json : Layer layer1.1.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer1.1.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer1.1.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer1.2.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer1.2.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer1.2.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer2.0.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer2.0.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer2.0.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer2.0.downsample.0, precision switch from w8a8 to w8a8.
Json : Layer layer2.1.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer2.1.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer2.1.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer2.2.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer2.2.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer2.2.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer2.3.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer2.3.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer2.3.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer3.0.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer3.0.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer3.0.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer3.0.downsample.0, precision switch from w8a8 to w8a8.
Json : Layer layer3.1.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer3.1.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer3.1.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer3.2.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer3.2.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer3.2.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer3.3.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer3.3.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer3.3.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer3.4.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer3.4.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer3.4.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer3.5.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer3.5.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer3.5.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer4.0.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer4.0.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer4.0.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer4.0.downsample.0, precision switch from w8a8 to w8a8.
Json : Layer layer4.1.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer4.1.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer4.1.conv3, precision switch from w8a8 to w8a8.
Json : Layer layer4.2.conv1, precision switch from w8a8 to w8a8.
Json : Layer layer4.2.conv2, precision switch from w8a8 to w8a8.
Json : Layer layer4.2.conv3, precision switch from w8a8 to w8a8.
Json : Layer fc, precision switch from w8a8 to w8a8.
step 3: 
Layer conv1, precision switch from w8a8 to w8a8.
Layer layer1.0.conv1, precision switch from w8a8 to w8a8.
Layer layer1.0.conv2, precision switch from w8a8 to w8a8.
Layer layer1.0.conv3, precision switch from w8a8 to w8a8.
Layer layer1.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer1.1.conv1, precision switch from w8a8 to w8a8.
Layer layer1.1.conv2, precision switch from w8a8 to w8a8.
Layer layer1.1.conv3, precision switch from w8a8 to w8a8.
Layer layer1.2.conv1, precision switch from w8a8 to w8a8.
Layer layer1.2.conv2, precision switch from w8a8 to w8a8.
Layer layer1.2.conv3, precision switch from w8a8 to w8a8.
Layer layer2.0.conv1, precision switch from w8a8 to w8a8.
Layer layer2.0.conv2, precision switch from w8a8 to w8a8.
Layer layer2.0.conv3, precision switch from w8a8 to w8a8.
Layer layer2.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer2.1.conv1, precision switch from w8a8 to w8a8.
Layer layer2.1.conv2, precision switch from w8a8 to w8a8.
Layer layer2.1.conv3, precision switch from w8a8 to w8a8.
Layer layer2.2.conv1, precision switch from w8a8 to w8a8.
Layer layer2.2.conv2, precision switch from w8a8 to w8a8.
Layer layer2.2.conv3, precision switch from w8a8 to w8a8.
Layer layer2.3.conv1, precision switch from w8a8 to w8a8.
Layer layer2.3.conv2, precision switch from w8a8 to w8a8.
Layer layer2.3.conv3, precision switch from w8a8 to w8a8.
Layer layer3.0.conv1, precision switch from w8a8 to w8a8.
Layer layer3.0.conv2, precision switch from w8a8 to w8a8.
Layer layer3.0.conv3, precision switch from w8a8 to w8a8.
Layer layer3.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer3.1.conv1, precision switch from w8a8 to w8a8.
Layer layer3.1.conv2, precision switch from w8a8 to w8a8.
Layer layer3.1.conv3, precision switch from w8a8 to w8a8.
Layer layer3.2.conv1, precision switch from w8a8 to w8a8.
Layer layer3.2.conv2, precision switch from w8a8 to w8a8.
Layer layer3.2.conv3, precision switch from w8a8 to w8a8.
Layer layer3.3.conv1, precision switch from w8a8 to w8a8.
Layer layer3.3.conv2, precision switch from w8a8 to w8a8.
Layer layer3.3.conv3, precision switch from w8a8 to w8a8.
Layer layer3.4.conv1, precision switch from w8a8 to w8a8.
Layer layer3.4.conv2, precision switch from w8a8 to w8a8.
Layer layer3.4.conv3, precision switch from w8a8 to w8a8.
Layer layer3.5.conv1, precision switch from w8a8 to w8a8.
Layer layer3.5.conv2, precision switch from w8a8 to w8a8.
Layer layer3.5.conv3, precision switch from w8a8 to w8a8.
Layer layer4.0.conv1, precision switch from w8a8 to w8a8.
Layer layer4.0.conv2, precision switch from w8a8 to w8a8.
Layer layer4.0.conv3, precision switch from w8a8 to w8a8.
Layer layer4.0.downsample.0, precision switch from w8a8 to w8a8.
Layer layer4.1.conv1, precision switch from w8a8 to w8a8.
Layer layer4.1.conv2, precision switch from w8a8 to w8a8.
Layer layer4.1.conv3, precision switch from w8a8 to w8a8.
Layer layer4.2.conv1, precision switch from w8a8 to w8a8.
Layer layer4.2.conv2, precision switch from w8a8 to w8a8.
Layer layer4.2.conv3, precision switch from w8a8 to w8a8.
Layer fc, precision switch from w8a8 to w8a8.
Input/outputs cached

Starting ONNX export with onnx 1.11.0...
****onnx file**** /workspace/develop/CalibTIP/results/resnet50_w8a8.adaquant/resnet.absorb_bn.measure_perC.before_adaquant.onnx
ONNX export success, saved as /workspace/develop/CalibTIP/results/resnet50_w8a8.adaquant/resnet.absorb_bn.measure_perC.before_adaquant.onnx
Bernard calculate float point model accuracy before enabling quantization!
Bernard calculate fixed point model accuracy before training!

Optimize 0:conv1 for w8a8 bit of shape torch.Size([64, 3, 7, 7])

MSE before adaquant: 7.240036e-05  RELU True
MSE after  adaquant: 2.815377e-05
 End of training layer  conv1 better/total layers= 1 / 1 


Optimize 1:layer1.0.conv1 for w8a8 bit of shape torch.Size([64, 64, 1, 1])

MSE before adaquant: 9.707782e-05  RELU True
MSE after  adaquant: 7.692066e-05
 End of training layer  layer1.0.conv1 better/total layers= 2 / 2 


Optimize 2:layer1.0.conv2 for w8a8 bit of shape torch.Size([64, 64, 3, 3])

MSE before adaquant: 2.357030e-05  RELU True
MSE after  adaquant: 6.595996e-06
 End of training layer  layer1.0.conv2 better/total layers= 3 / 3 


Optimize 3:layer1.0.conv3 for w8a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 1.651893e-05  RELU False
MSE after  adaquant: 8.805137e-06
 End of training layer  layer1.0.conv3 better/total layers= 4 / 4 


Optimize 4:layer1.0.downsample.0 for w8a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 1.347184e-04  RELU False
MSE after  adaquant: 1.119310e-04
 End of training layer  layer1.0.downsample.0 better/total layers= 5 / 5 


Optimize 5:layer1.1.conv1 for w8a8 bit of shape torch.Size([64, 256, 1, 1])

MSE before adaquant: 2.438001e-05  RELU True
MSE after  adaquant: 1.730897e-05
 End of training layer  layer1.1.conv1 better/total layers= 6 / 6 


Optimize 6:layer1.1.conv2 for w8a8 bit of shape torch.Size([64, 64, 3, 3])

MSE before adaquant: 1.212310e-05  RELU True
MSE after  adaquant: 6.501928e-06
 End of training layer  layer1.1.conv2 better/total layers= 7 / 7 


Optimize 7:layer1.1.conv3 for w8a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 8.111429e-06  RELU False
MSE after  adaquant: 6.999040e-06
 End of training layer  layer1.1.conv3 better/total layers= 8 / 8 


Optimize 8:layer1.2.conv1 for w8a8 bit of shape torch.Size([64, 256, 1, 1])

MSE before adaquant: 3.381995e-05  RELU True
MSE after  adaquant: 2.800613e-05
 End of training layer  layer1.2.conv1 better/total layers= 9 / 9 


Optimize 9:layer1.2.conv2 for w8a8 bit of shape torch.Size([64, 64, 3, 3])

MSE before adaquant: 6.923433e-06  RELU True
MSE after  adaquant: 5.032462e-06
 End of training layer  layer1.2.conv2 better/total layers= 10 / 10 


Optimize 10:layer1.2.conv3 for w8a8 bit of shape torch.Size([256, 64, 1, 1])

MSE before adaquant: 2.633148e-05  RELU False
MSE after  adaquant: 2.594075e-05
 End of training layer  layer1.2.conv3 better/total layers= 11 / 11 


Optimize 11:layer2.0.conv1 for w8a8 bit of shape torch.Size([128, 256, 1, 1])

MSE before adaquant: 1.638838e-05  RELU True
MSE after  adaquant: 1.111486e-05
 End of training layer  layer2.0.conv1 better/total layers= 12 / 12 


Optimize 12:layer2.0.conv2 for w8a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 5.498824e-06  RELU True
MSE after  adaquant: 4.167036e-06
 End of training layer  layer2.0.conv2 better/total layers= 13 / 13 


Optimize 13:layer2.0.conv3 for w8a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 1.081050e-05  RELU False
MSE after  adaquant: 9.013099e-06
 End of training layer  layer2.0.conv3 better/total layers= 14 / 14 


Optimize 14:layer2.0.downsample.0 for w8a8 bit of shape torch.Size([512, 256, 1, 1])

MSE before adaquant: 3.935748e-04  RELU False
MSE after  adaquant: 3.725673e-04
 End of training layer  layer2.0.downsample.0 better/total layers= 15 / 15 


Optimize 15:layer2.1.conv1 for w8a8 bit of shape torch.Size([128, 512, 1, 1])

MSE before adaquant: 1.764070e-05  RELU True
MSE after  adaquant: 1.259361e-05
 End of training layer  layer2.1.conv1 better/total layers= 16 / 16 


Optimize 16:layer2.1.conv2 for w8a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 1.389838e-04  RELU True
MSE after  adaquant: 1.049553e-04
 End of training layer  layer2.1.conv2 better/total layers= 17 / 17 


Optimize 17:layer2.1.conv3 for w8a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 5.114613e-05  RELU False
MSE after  adaquant: 4.980336e-05
 End of training layer  layer2.1.conv3 better/total layers= 18 / 18 


Optimize 18:layer2.2.conv1 for w8a8 bit of shape torch.Size([128, 512, 1, 1])

MSE before adaquant: 1.148976e-05  RELU True
MSE after  adaquant: 9.185379e-06
 End of training layer  layer2.2.conv1 better/total layers= 19 / 19 


Optimize 19:layer2.2.conv2 for w8a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 6.703189e-06  RELU True
MSE after  adaquant: 4.556116e-06
 End of training layer  layer2.2.conv2 better/total layers= 20 / 20 


Optimize 20:layer2.2.conv3 for w8a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 7.261773e-06  RELU False
MSE after  adaquant: 6.331469e-06
 End of training layer  layer2.2.conv3 better/total layers= 21 / 21 


Optimize 21:layer2.3.conv1 for w8a8 bit of shape torch.Size([128, 512, 1, 1])

MSE before adaquant: 9.069531e-06  RELU True
MSE after  adaquant: 7.535501e-06
 End of training layer  layer2.3.conv1 better/total layers= 22 / 22 


Optimize 22:layer2.3.conv2 for w8a8 bit of shape torch.Size([128, 128, 3, 3])

MSE before adaquant: 4.180426e-06  RELU True
MSE after  adaquant: 3.260993e-06
 End of training layer  layer2.3.conv2 better/total layers= 23 / 23 


Optimize 23:layer2.3.conv3 for w8a8 bit of shape torch.Size([512, 128, 1, 1])

MSE before adaquant: 5.950713e-06  RELU False
MSE after  adaquant: 5.529992e-06
 End of training layer  layer2.3.conv3 better/total layers= 24 / 24 


Optimize 24:layer3.0.conv1 for w8a8 bit of shape torch.Size([256, 512, 1, 1])

MSE before adaquant: 2.869224e-05  RELU True
MSE after  adaquant: 2.643734e-05
 End of training layer  layer3.0.conv1 better/total layers= 25 / 25 


Optimize 25:layer3.0.conv2 for w8a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 9.213219e-06  RELU True
MSE after  adaquant: 7.841511e-06
 End of training layer  layer3.0.conv2 better/total layers= 26 / 26 


Optimize 26:layer3.0.conv3 for w8a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 1.204067e-05  RELU False
MSE after  adaquant: 9.612208e-06
 End of training layer  layer3.0.conv3 better/total layers= 27 / 27 


Optimize 27:layer3.0.downsample.0 for w8a8 bit of shape torch.Size([1024, 512, 1, 1])

MSE before adaquant: 1.133784e-05  RELU False
MSE after  adaquant: 9.863131e-06
 End of training layer  layer3.0.downsample.0 better/total layers= 28 / 28 


Optimize 28:layer3.1.conv1 for w8a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 4.693164e-06  RELU True
MSE after  adaquant: 3.678793e-06
 End of training layer  layer3.1.conv1 better/total layers= 29 / 29 


Optimize 29:layer3.1.conv2 for w8a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 1.256161e-05  RELU True
MSE after  adaquant: 1.105402e-05
 End of training layer  layer3.1.conv2 better/total layers= 30 / 30 


Optimize 30:layer3.1.conv3 for w8a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 1.675426e-05  RELU False
MSE after  adaquant: 1.616148e-05
 End of training layer  layer3.1.conv3 better/total layers= 31 / 31 


Optimize 31:layer3.2.conv1 for w8a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 1.849068e-05  RELU True
MSE after  adaquant: 1.594228e-05
 End of training layer  layer3.2.conv1 better/total layers= 32 / 32 


Optimize 32:layer3.2.conv2 for w8a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 3.023910e-06  RELU True
MSE after  adaquant: 2.368348e-06
 End of training layer  layer3.2.conv2 better/total layers= 33 / 33 


Optimize 33:layer3.2.conv3 for w8a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 4.124139e-06  RELU False
MSE after  adaquant: 3.835721e-06
 End of training layer  layer3.2.conv3 better/total layers= 34 / 34 


Optimize 34:layer3.3.conv1 for w8a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 1.517845e-05  RELU True
MSE after  adaquant: 1.321711e-05
 End of training layer  layer3.3.conv1 better/total layers= 35 / 35 


Optimize 35:layer3.3.conv2 for w8a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 4.717115e-06  RELU True
MSE after  adaquant: 3.847907e-06
 End of training layer  layer3.3.conv2 better/total layers= 36 / 36 


Optimize 36:layer3.3.conv3 for w8a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 5.487240e-06  RELU False
MSE after  adaquant: 5.244079e-06
 End of training layer  layer3.3.conv3 better/total layers= 37 / 37 


Optimize 37:layer3.4.conv1 for w8a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 5.804465e-05  RELU True
MSE after  adaquant: 5.102723e-05
 End of training layer  layer3.4.conv1 better/total layers= 38 / 38 


Optimize 38:layer3.4.conv2 for w8a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 5.428723e-06  RELU True
MSE after  adaquant: 4.795937e-06
 End of training layer  layer3.4.conv2 better/total layers= 39 / 39 


Optimize 39:layer3.4.conv3 for w8a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 1.226063e-05  RELU False
MSE after  adaquant: 1.188724e-05
 End of training layer  layer3.4.conv3 better/total layers= 40 / 40 


Optimize 40:layer3.5.conv1 for w8a8 bit of shape torch.Size([256, 1024, 1, 1])

MSE before adaquant: 1.166761e-04  RELU True
MSE after  adaquant: 1.042292e-04
 End of training layer  layer3.5.conv1 better/total layers= 41 / 41 


Optimize 41:layer3.5.conv2 for w8a8 bit of shape torch.Size([256, 256, 3, 3])

MSE before adaquant: 1.382122e-05  RELU True
MSE after  adaquant: 1.201958e-05
 End of training layer  layer3.5.conv2 better/total layers= 42 / 42 


Optimize 42:layer3.5.conv3 for w8a8 bit of shape torch.Size([1024, 256, 1, 1])

MSE before adaquant: 7.591194e-05  RELU False
MSE after  adaquant: 7.391240e-05
 End of training layer  layer3.5.conv3 better/total layers= 43 / 43 


Optimize 43:layer4.0.conv1 for w8a8 bit of shape torch.Size([512, 1024, 1, 1])

MSE before adaquant: 1.377778e-05  RELU True
MSE after  adaquant: 1.322381e-05
 End of training layer  layer4.0.conv1 better/total layers= 44 / 44 


Optimize 44:layer4.0.conv2 for w8a8 bit of shape torch.Size([512, 512, 3, 3])

MSE before adaquant: 1.002979e-05  RELU True
MSE after  adaquant: 9.509901e-06
 End of training layer  layer4.0.conv2 better/total layers= 45 / 45 


Optimize 45:layer4.0.conv3 for w8a8 bit of shape torch.Size([2048, 512, 1, 1])

MSE before adaquant: 2.974846e-04  RELU False
MSE after  adaquant: 2.903426e-04
 End of training layer  layer4.0.conv3 better/total layers= 46 / 46 


Optimize 46:layer4.0.downsample.0 for w8a8 bit of shape torch.Size([2048, 1024, 1, 1])

MSE before adaquant: 9.212171e-05  RELU False
MSE after  adaquant: 8.807777e-05
 End of training layer  layer4.0.downsample.0 better/total layers= 47 / 47 


Optimize 47:layer4.1.conv1 for w8a8 bit of shape torch.Size([512, 2048, 1, 1])

MSE before adaquant: 1.123623e-05  RELU True
MSE after  adaquant: 8.509955e-06
 End of training layer  layer4.1.conv1 better/total layers= 48 / 48 


Optimize 48:layer4.1.conv2 for w8a8 bit of shape torch.Size([512, 512, 3, 3])

MSE before adaquant: 7.499167e-06  RELU True
MSE after  adaquant: 7.215661e-06
 End of training layer  layer4.1.conv2 better/total layers= 49 / 49 


Optimize 49:layer4.1.conv3 for w8a8 bit of shape torch.Size([2048, 512, 1, 1])

MSE before adaquant: 1.984170e-04  RELU False
MSE after  adaquant: 1.965743e-04
 End of training layer  layer4.1.conv3 better/total layers= 50 / 50 


Optimize 50:layer4.2.conv1 for w8a8 bit of shape torch.Size([512, 2048, 1, 1])

MSE before adaquant: 2.791550e-05  RELU True
MSE after  adaquant: 2.167468e-05
 End of training layer  layer4.2.conv1 better/total layers= 51 / 51 


Optimize 51:layer4.2.conv2 for w8a8 bit of shape torch.Size([512, 512, 3, 3])

MSE before adaquant: 5.648316e-06  RELU True
MSE after  adaquant: 5.350866e-06
 End of training layer  layer4.2.conv2 better/total layers= 52 / 52 


Optimize 52:layer4.2.conv3 for w8a8 bit of shape torch.Size([2048, 512, 1, 1])

MSE before adaquant: 7.923805e-04  RELU False
MSE after  adaquant: 7.797198e-04
 End of training layer  layer4.2.conv3 better/total layers= 53 / 53 


Optimize 53:fc for w8a8 bit of shape torch.Size([1000, 2048])

MSE before adaquant: 1.990091e-03  RELU False
MSE after  adaquant: 1.131624e-03
 End of training layer  fc better/total layers= 54 / 54 

